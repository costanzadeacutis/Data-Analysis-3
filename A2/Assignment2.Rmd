---
title: "airbnb-milano-analysis"
author: "costanzadeacutis"
date: "2023-02-01"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
rm(list=ls())

source("functions.R")
source("theme_bg.R")

# Import libraries
library(tidyverse)
library(lspline)
library(cowplot)
library(ggplot2)
library(boot)
library(estimatr)
library(huxtable)
library(stargazer)
library(modelsummary)
library(janitor)
library(fixest)
library(grid)
library(caret)
library(skimr)
library(psych)
library(directlabels)
library(xtable)
library(glmnet)
library(knitr)
library(dplyr)
library(kableExtra)
library(ranger)
knitr::opts_chunk$set(echo = TRUE)

#folder to save graphs, files .tex etc
output <- "output/"
```

# Setup
For the purpose of the analysis, I retain only those that can be defined as apartments in terms of property type.

I create factors from room_type variable, and I categorize it as factor variable together with f_municipio.
I remove apartments that are categorized as outside, i.e. not in one of the 9 municipi, hence not directly comparable with the others (I assume that the apartments whose prices I am trying to predict are all in the city).
I also retain only those apartments that can accommodate 2-6 guests, as the new apartments in question.

NB: Municipio - Given that the neighboorhoods are too many and too small, I regrouped them in "municipi": this categorization comes from comune.milano.it. I then factorize them.
Note: municipio_2_9 is area between Municipio 2 and Municipio 9, that could not be categorized in any of the two.

```{r data}
data <- read.csv("airbnb_milano_cleaned.csv")

#check property types
table(data$property_type)
#keep only apartments
#excluded private room cause not clear
data <- data %>% filter(property_type %in% c("Entire condo", "Entire home", "Entire home/apt", "Entire loft", "Entire place", "Entire rental unit", "Entire serviced apartment", "Entire vacation home", "Private room in condo", "Private room in home", "Private room in loft", "Private room in rental unit", "Private room in serviced apartment", "Private room in tiny home", "Private room in vacation home", "Room in serviced apartment", "Shared room in condo", "Shared room in home", "Shared room in loft", "Shared room in rental unit", "Tiny home"))

#check room types
table(data$room_type)
#remove hotel room
data <- data %>% filter(room_type %in% c("Entire home/apt", "Private room", "Shared room"))
#factor
data <- data %>% mutate(f_room_type = factor(room_type))

#categorical variables
categoricals <- c("f_room_type", "f_municipio")

for (i in 1:length(categoricals)) {
  data %>%
    group_by(get(categoricals[i])) %>%
    summarise(mean_price = mean(price) ,  n=n()) %>%
    print
}

#drop "outside" (not in the city center)
data <- data %>% filter(!(f_municipio %in% c("Outside")))

#number accommodates
data <- data %>% filter(n_accommodates %in% c(2,3,4,5,6))

#drop unused first column
drops <- c("X")
data <- data[ , !(names(data) %in% drops)]
```

## First look at the data
I have a dataset with 16833 observations and 77 variables. Some of these variables have some/many missing values: review_scores_rate, p_host_acceptance_rate, p_host_response_rate, n_days_since, n_bathrooms, f_bathrooms_type, n_beds, n_bedrooms, n_reviews_per_month

```{r first look}
glimpse(data)
skim(data)

# where do we have missing variables now?
to_filter <- sapply(data, function(x) sum(is.na(x)))
to_filter[to_filter > 0]
```

## Further cleaning - Deal with NAs
There are different ways of dealing with missing values:
1. remove when it is the target variable
2. remove those variables that have too many NAs and the variable is not that important for the analysis
3. input values (for beds, assume that the number is the same as the number of accommodates)
4. replace with median and flag when there are many but the variable is important for the analysis
```{r cleaning}
# 1. drop NA if target variable
data <- data %>%
  drop_na(price)

# 2. remove when too many
to_drop <- c("f_bathrooms_type", "p_host_response_rate", "p_host_acceptance_rate", "property_type")
data <- data %>%
  select(-one_of(to_drop))

# 3. imput when few, not that important
#bathrooms
#beds
data <- data %>%
  mutate(
    n_bathrooms =  ifelse(is.na(n_bathrooms), median(n_bathrooms, na.rm = T), n_bathrooms),
    n_beds = ifelse(is.na(n_beds), n_accommodates, n_beds)) #assume n_beds=n_accommodates

to_filter <- sapply(data, function(x) sum(is.na(x)))
to_filter[to_filter > 0]


# 4. Replace missing variables + add flags
#reviews
#bedrooms
#days since
data <- data %>%
  mutate(flag_bedrooms=ifelse(is.na(n_bedrooms),1, 0),
    n_bedrooms =  ifelse(is.na(n_bedrooms), median(n_bedrooms, na.rm = T), n_bedrooms),
    flag_days_since=ifelse(is.na(n_days_since),1, 0),
    n_days_since =  ifelse(is.na(n_days_since), median(n_days_since, na.rm = T), n_days_since),
    flag_review_scores_rating=ifelse(is.na(review_scores_rating),1, 0),
    review_scores_rating =  ifelse(is.na(review_scores_rating), median(review_scores_rating, na.rm = T), review_scores_rating),
    flag_reviews_per_month=ifelse(is.na(n_reviews_per_month),1, 0),
    n_reviews_per_month =  ifelse(is.na(n_reviews_per_month), median(n_reviews_per_month, na.rm = T), n_reviews_per_month)
          )
table(data$flag_days_since)
```

## Create variables polynomials
- measuring the time since: squared, cubic, logs
- squared accommodates
- log ratings
```{r create variables}
data <- data %>%
  mutate(
    ln_days_since = log(n_days_since+1),
    ln_days_since2 = log(n_days_since+1)^2,
    ln_days_since3 = log(n_days_since+1)^3 ,
    n_days_since2=n_days_since^2,
    n_days_since3=n_days_since^3,
    n_accommodates2 = n_accommodates^2,
    ln_review_scores_rating = log(review_scores_rating),
    ln_days_since=ifelse(is.na(ln_days_since),0, ln_days_since),
    ln_days_since2=ifelse(is.na(ln_days_since2),0, ln_days_since2),
    ln_days_since3=ifelse(is.na(ln_days_since3),0, ln_days_since3),
  )
```


# Descriptive analysis
## Prices
The prices for these apartments range from 9 to 90180 dollars per night. Probably both the very low and the very high prices are either anomalies or were wrongly imputed, especially the very high ones. Very extreme prices would be difficult to predict.
Given these considerations, I retain only those prices above 20 and below 1500 dollars per night. The resulting dataset has 16700 observations and 90 variables, after some further data cleaning and feature engineering.
I first look at how the average price moves in terms of Municipio and room type. As expected, highest prices are in Municipio 1 (which is the core of the city) and for entire apartments/home.
I analyze the histograms for prices in levels and logs: the distribution of prices in levels is right-skewed, meaning that there are very few high prices; the log transformation gives a normally shaped distribution. Moreover, for better visualization, I only prices below 400 dollars per night, and this time I look at percentages. The same evaluation as before applies.
Finally I look at apartment price distribution by room type: it is clear that the entire apartments are priced the highest, the shared rooms the lowest, as expected.

```{r prices, warning=FALSE}
#Prices
summary(data$price)
describe(data$price)

#drop very weird prices above 1500 and below 20
data <- data %>% filter(price <= 1500,
                        price > 20)

data <- data %>%
  mutate(ln_price = log(price))

#How is the average price changing by `room_type` and the `municipio`?
data %>%
  group_by(f_municipio, f_room_type) %>%
  dplyr::summarize(mean_price = mean(price, na.rm=TRUE))

Hmisc::describe(data$price)

# Histograms
Airbnb_price <- ggplot(data, aes(price)) +
  geom_histogram(binwidth = 25, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("count") +
  xlab("Price") +
  theme_bg()
Airbnb_price

Airbnb_lnprice <- ggplot(data, aes(ln_price)) +
  geom_histogram(binwidth = 0.15, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("Count") +
  xlab("Log price") +
  theme_bg()
Airbnb_lnprice

# Look even closer, consider only prices below 400
datau <- subset(data, price<400)

# Histograms
# price
Airbnb_price_percent <- ggplot(data=datau, aes(x=price)) +
  geom_histogram_da(type="percent", binwidth = 10) +
  labs(x = "Price (US dollars)",y = "Percent")+
  scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.15), breaks = seq(0, 0.15, by = 0.03), labels = scales::percent_format(1)) +
    scale_x_continuous(expand = c(0.00,0.00),limits=c(0,400), breaks = seq(0,400, 50)) +
  theme_bw() 
Airbnb_price_percent
save_fig("Airbnb-price-percent", output, "small")


# lnprice
Airbnb_price_percent<- ggplot(data=datau, aes(x=ln_price)) +
  geom_histogram_da(type="percent", binwidth = 0.18) +
  #  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 0.18,
  #               color = color.outline, fill = color[1], size = 0.25, alpha = 0.8,  show.legend=F,  na.rm=TRUE) +
  coord_cartesian(xlim = c(2.5, 6.5)) +
  scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.15), breaks = seq(0, 0.15, by = 0.05), labels = scales::percent_format(5L)) +
  scale_x_continuous(expand = c(0.00,0.01),breaks = seq(2.4,6.6, 0.6)) +
  labs(x = "ln(price, US dollars)",y = "Percent")+
  theme_bw() 
Airbnb_price_percent
save_fig("Airbnb-price-percent", output, "small")


## Boxplot of price by room type
Airbnb_room_type <- ggplot(data = datau, aes(x = f_room_type, y = price)) +
  stat_boxplot(aes(group = f_room_type), geom = "errorbar", width = 0.3,
               color = c(color[2],color[1], color[3]), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = f_room_type),
               color = c(color[2],color[1], color[3]), fill = c(color[2],color[1], color[3]),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,400), breaks = seq(0,400,100)) +
  labs(x = "Room type",y = "Price (US dollars)")+
  theme_bg()
Airbnb_room_type
save_fig("Airbnb_room_type", output, "small")

```

## Number of accommodates
It can be seen from both the graph and the table that the mean price increases as the number of accommodates increase, as expected.

```{r accommodates}
## n_accommodates: look at distribution

data %>%
  group_by(n_accommodates) %>%
  summarise(mean_price = mean(price), min_price= min(price), max_price = max(price), n = n())

Airbnb_n_accommodates <- ggplot(data = data, aes(x=n_accommodates, y=price)) +
  geom_point(size=1, colour=color[3], shape=16)+
  ylim(0,1500)+
  xlim(2,6)+
  labs(x="Number of people accomodated",y="Price")+
  geom_smooth(method="lm", colour=color[1], se=FALSE)+
  theme_bg()
Airbnb_n_accommodates
```

## Number of beds and bathrooms
The number of beds ranges from 1 to 10 for these apartments. Mean prices tend to increase as the number of beds increases, with some exceptions: 8 and 9 beds have lower or equal mean prices compared to 1 or 2 beds. This is could be due to overcrowding: more beds but the meters squared do not increase.

For bathrooms, where the value is 0, it probably means that the bathroom is outside the private or shared room, thus I impute that there is at least 1. Also those 0.5, I assume that they are like 1 bathroom.
The mean price increases as the number of bathrooms increases.

```{r beds and bathrooms}
## Beds
data %>%
  group_by(n_beds) %>%
  summarise(mean_price = mean(price), min_price= min(price), max_price = max(price), n = n())

# maybe best is to have log beds
data <- data %>%
  mutate(ln_beds = log(n_beds))

ggplot(data, aes(n_beds)) +
  geom_histogram(binwidth = 0.5, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("N of beds") +
  theme_bg()

## bathrooms
data %>%
  group_by(n_bathrooms) %>%
  summarise(mean_price = mean(price), n = n())

data <- data %>%
  mutate(n_bathrooms =  ifelse(n_bathrooms==0.0, 1, n_bathrooms),
         n_bathrooms =  ifelse(n_bathrooms==0.5, 1, n_bathrooms))

ggplot(data, aes(n_bathrooms)) +
  geom_histogram(binwidth = 0.5, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("N of bathrooms") +
  theme_bg()

data %>%
  group_by(n_bathrooms) %>%
  summarise(mean_price = mean(price), n = n())


```

## Number of reviews
For better visualization, I consider only those apartments that have less then 100 reviews (those with more are not frequent). The distribution is highly right-skewed.
I also create a factor variable for 3 categories: apartments with either none, 1-50, or 51+ reviews. Apartments with 0 reviews appear to have higher mean prices.

```{r number of reviews}
## Number of reviews

ggplot(data, aes(n_number_of_reviews)) +
  geom_histogram(binwidth = 10, fill = 'navyblue', color = 'white', alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("N of reviews") +
  theme_bw()

nreview_plot <- data %>%
  filter(n_number_of_reviews <100)

ggplot(nreview_plot, aes(n_number_of_reviews)) +
  geom_histogram(binwidth = 5, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("N of reviews") +
  theme_bg()

# number of reviews: use logs as well
data <- data %>%
  mutate(ln_number_of_reviews = log(n_number_of_reviews+1))

ggplot(data, aes(ln_number_of_reviews)) +
  geom_histogram(binwidth = 0.5, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("Log N of reviews") +
  theme_bg()

# Pool num of reviews to 3 categories: none, 1-51 and >51
data <- data %>%
  mutate(f_number_of_reviews = cut(n_number_of_reviews, c(0,1,51,max(data$n_number_of_reviews)+1), labels=c(0,1,2), right = F))

data %>%
  group_by(f_number_of_reviews) %>%
  summarise(median_price = median(price), mean_price = mean(price) ,  n=n())


```

## Review scores rating
From the graph, most of the apartments have a high review score, but there is no clear relation with daily prices.

```{r review scores}
## review score effect
data %>%
  group_by(review_scores_rating) %>%
  summarise(mean_price = mean(price), min_price= min(price), max_price = max(price), n = n())

ggplot(data = data, aes(x=review_scores_rating , y=price)) +
  geom_point(size=1.5, colour=color[3], shape=4) +
  ylim(0,1500)+
  xlim(0,5)+
  geom_smooth(method="loess", colour=color[1], se=F)+
  labs(x="Review score",y="Daily price (USD)")+
  theme_bg()

```

## Minimum nights
It is not specified if the apartments in this project are intended for short or long stays; however, usually Airbnbs are mostly for short stays. Therefore, I created a factor for 4 categories: 1, 2, 3-7, 8+ minimum nights. 

```{r minimum nights}
# minimum nights
#not specified if only short stays

data %>%
  group_by(n_minimum_nights) %>%
  summarise(mean_price = mean(price), min_price= min(price), max_price = max(price), n = n())

data <- data %>%
  mutate(f_minimum_nights= cut(n_minimum_nights, c(1,2,3,7,max(data$n_minimum_nights)+1), labels=c(1,2,3,4), right = F))

data %>%
  group_by(f_minimum_nights) %>%
  summarise(mean_price = mean(price), min_price= min(price), max_price = max(price), n = n())

```





# Models
My analysis starts by looking at potential interactions: 
1. Minimum nights with children friendly amenities, presence of a gym, kitchen amenities, outdoor furniture
2. Room type with elevator, kitchen amenities, wardrobe, special view
3. Bathrooms with bathtub, hot tub, bathroom essentials, essentials
From the bar charts, it can be seen that do not emerge very clear interactions

```{r interactions, warning=FALSE}

#Look up min nights interactions
p1 <- price_diff_by_variables2(data, "f_minimum_nights", "d_babykit", "Minimum nights", "Children friendly")
p2 <- price_diff_by_variables2(data, "f_minimum_nights", "d_gym", "Minimum nights", "Gym")

p3 <- price_diff_by_variables2(data, "f_minimum_nights", "d_kitchenamenities", "Minimum nights", "Kitchen amenities")
p4 <- price_diff_by_variables2(data, "f_minimum_nights", "d_outdoorfurniture", "Minimum nights", "Outdoor furniture")

g_interactions <- plot_grid(p1, p2, p3, p4, nrow=2, ncol=2)
g_interactions
save_fig("interactions_min_nights",output,"large")

#Look up room types interactions
p5 <- price_diff_by_variables2(data, "f_room_type", "d_elevator", "Room type", "Elevator")
p6 <- price_diff_by_variables2(data, "f_room_type", "d_kitchenamenities", "Room type", "Kitchen amenities")

p7 <- price_diff_by_variables2(data, "f_room_type", "d_wardrobe", "Room type", "Wardrobe")
p8 <- price_diff_by_variables2(data, "f_room_type", "d_specialview", "Room type", "Special view")

g_interactions2 <- plot_grid(p5, p6, p7, p8, nrow=2, ncol=2)
g_interactions2
save_fig("interactions_room_type",output,"large")

#Look up bathrooms interactions
p9 <- price_diff_by_variables2(data, "n_bathrooms", "d_bathtub", "Bathrooms", "Bathtub")
p10 <- price_diff_by_variables2(data, "n_bathrooms", "d_hottub", "Bathrooms", "Hot tub")

p11 <- price_diff_by_variables2(data, "n_bathrooms", "d_bathroomessentials", "Bathrooms", "Bathroom essentials")
p12 <- price_diff_by_variables2(data, "n_bathrooms", "d_essentials", "Bathrooms", "Essentials")


g_interactions3 <- plot_grid(p9, p10, p11, p12, nrow=2, ncol=2)
g_interactions3
save_fig("interactions_bathroom",output,"large")


```

## General setup
I group variables in basic variables, additional variables, review variables, and variables or higher order. These are the variables that I use in the analysis. Moreover, I put a d_ in front of all amenities dummies.
Then, I randomly create:
- Work set (data_work, 80% of the total data): where I will do the model building, the selection of the best model and the prediction;
- Holdout set (data_holdout, 20% of the total data): part of the data for evaluating the prediction itself
Finally, I create interactions following the graphs and interactions of variables that I believe are more relevant with the amenities. 

```{r setup models}

# Basic Variables
basic_lev  <- c("n_accommodates", "f_room_type", "f_municipio")
# Additional variables
basic_add <- c("n_beds", "n_bathrooms", "f_minimum_nights", "n_days_since", "flag_days_since")
reviews <- c("f_number_of_reviews","review_scores_rating", "flag_review_scores_rating")
# Higher orders
poly_lev <- c("n_accommodates2", "n_days_since2", "n_days_since3")

# Dummy variables: Extras -> collect all options and create dummies
amenities <-  grep("^d_.*", names(data), value = TRUE)


# Separate the Holdout set, and obtain the Working data

# create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data))

# Set the random number generator: It will make results reproducible
set.seed(20180123)

# create ids:
# 1) seq_len: generate regular sequences
# 2) sample: select random rows from a table
holdout_ids <- sample(seq_len(nrow(data)), size = smp_size)
data$holdout <- 0
data$holdout[holdout_ids] <- 1

#Hold-out set Set
data_holdout <- data %>% filter(holdout == 1)

#Working data set
data_work <- data %>% filter(holdout == 0)

#Interactions suggested by the graphs
X1 <- c("f_minimum_nights*(d_babykit+d_outdoorfurniture+d_gym)")
#Additional interactions
X2  <- c(paste0("(f_room_type+f_municipio+f_minimum_nights+n_bathrooms) * (",
                paste(amenities, collapse=" + "),")"))

```

## OLS regressions with cross validation
In the following section, I construct 6 different models where prices are regressed on
- M1: number of accomodates
- M2: M1 + basic var + additional var + review var
- M3: M2 + polynomials
- M4: M3 + suggested interactions
- M5: M4 + amenities
- M6: M5 + additional interactions
Before running these regressions, I expect at least the 6th model to be too complex, i.e. too many coefficients.
I then set up a 5-fold cross-validation for model selection: the models are estimated on the training sample (data_train), and then the predictive performance is evaluated on the rest of the data, i.e. on the test sample (data_test). The aim is to avoid overfitting, i.e. better fit to original data, but worse with live data.
From the summary table, it can be seen that as the complexity increases, the RMSE, which measures how well the model predicts, decreases in the training set, while it first decreases and then increases in the test set. This is clearly shown by the graph. To chose the best model among those evaluated, I look at the lowest RMSE in the test set, which corresponds to model 5. While, more complex models as M6 are overfitting.
By choosing model 5, it seems that the relevant variables for the predictions are the those in basic, additional, reviews, amenities, and suggested interactions groups. 

```{r regressions, warning=FALSE}

# Create models in levels models: 1-8
modellev1 <- " ~ n_accommodates"
modellev2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
modellev3 <- paste0(" ~ ",paste(c(basic_lev, basic_add,reviews),collapse = " + "))
modellev4 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev),collapse = " + "))
modellev4 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev,X1),collapse = " + "))
modellev5 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev,X1,amenities),collapse = " + "))
modellev6 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev,X1,amenities,X2),collapse = " + "))


# Cross validation
## N = 5
n_folds=5
# Create the folds
set.seed(20180124)

folds_i <- sample(rep(1:n_folds, length.out = nrow(data_work) ))
# Create results
model_results_cv <- list()


for (i in (1:6)){
  model_name <-  paste0("modellev",i)
  model_pretty_name <- paste0("(",i,")")

  yvar <- "price"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))

  # Initialize values
  rmse_train <- c()
  rmse_test <- c()

  model_work_data <- lm(formula,data = data_work)
  BIC <- BIC(model_work_data)
  nvars <- model_work_data$rank -1
  r2 <- summary(model_work_data)$r.squared

  # Do the k-fold estimation
  for (k in 1:n_folds) {
    test_i <- which(folds_i == k)
    # Train sample: all except test_i
    data_train <- data_work[-test_i, ]
    # Test sample
    data_test <- data_work[test_i, ]
    # Estimation and prediction
    model <- lm(formula,data = data_train)
    prediction_train <- predict(model, newdata = data_train)
    prediction_test <- predict(model, newdata = data_test)

    # Criteria evaluation
    rmse_train[k] <- mse_lev(prediction_train, data_train[,yvar])**(1/2)
    rmse_test[k] <- mse_lev(prediction_test, data_test[,yvar])**(1/2)
    
  }
  
   model_results_cv[[model_name]] <- list(yvar=yvar,xvars=xvars,formula=formula,model_work_data=model_work_data,
                                         rmse_train = rmse_train,rmse_test = rmse_test,BIC = BIC,
                                         model_name = model_pretty_name, nvars = nvars, r2 = r2)
}


model <- lm(formula,data = data_train)
prediction_train <- predict(model, newdata = data_train)
prediction_test <- predict(model, newdata = data_test)

#skim(data_train$ln_days_since)

t1 <- imap(model_results_cv,  ~{
  as.data.frame(.x[c("rmse_test", "rmse_train")]) %>%
    dplyr::summarise_all(.funs = mean) %>%
    mutate("model_name" = .y , "model_pretty_name" = .x[["model_name"]] ,
           "nvars" = .x[["nvars"]], "r2" = .x[["r2"]], "BIC" = .x[["BIC"]])
}) %>%
  bind_rows()
t1
column_names <- c("Model", "N predictors", "R-squared", "BIC", "Training RMSE",
                 "Test RMSE")

# R2, BIC on full work data-n.
# In sample rmse: average on training data; avg test : average on test data

t14_2 <- t1 %>%
  select("model_pretty_name", "nvars", "r2" , "BIC", "rmse_train", "rmse_test")
colnames(t14_2) <- column_names
print(xtable(t14_2, type = "latex", digits=c(0,0,0,2,0,2,2)), include.rownames=FALSE, booktabs=TRUE, floating = FALSE)

# RMSE training vs test graph
t1_levels <- t1 %>%
  dplyr::select("nvars", "rmse_train", "rmse_test") %>%
  gather(var,value, rmse_train:rmse_test) %>%
  mutate(nvars2=nvars+1) %>%
  mutate(var = factor(var, levels = c("rmse_train", "rmse_test"),
                      labels = c("RMSE Training","RMSE Test")))

model_result_plot_levels <- ggplot(data = t1_levels,
                                   aes(x = factor(nvars2), y = value, color=factor(var), group = var)) +
  geom_line(size=1,show.legend=FALSE, na.rm = TRUE) +
  scale_color_manual(name="",
                     values=c(color[2],color[1])) +
  scale_y_continuous(name = "RMSE", limits = c(90, 114), breaks = seq(90,114, 2)) +
  scale_x_discrete( name = "Number of coefficients", expand=c(0.01, 0.01)) +
  geom_dl(aes(label = var),  method = list("last.points", dl.trans(x=x-1), cex=0.4)) +
  #scale_colour_discrete(guide = 'none') +
  theme_bg()
model_result_plot_levels
save_fig("Airbnb-model-result-levels", output, "small")

```

## Regression with LASSO
I now look into LASSO, which is an automatic method to select variables: it shrinks coefficients towards zero of those variables that variables whose inclusion does not improve the fit of the regression. This is equivalent to removing them from the regression.
I use M6 with 819 candidate features and I ran the LASSO algorithm with 5-fold cross-validation for selecting the optimal for the tuning parameter lambda (which gives the weight to the penalty term). The optimal lambda is 0.5 and the suggested features to be kept are 361: LASSO removed many features that were included in M6, but the number is greater than that in M5, which was chosen through cross-validation in the previous part. LASSO does slightly better in terms of prediction compared to M5 (101.7 against 102.5). It works only slightly better, which means that I can keep M5 as my preferred model: in fact, even if LASSO is fast and automatic, it is hard to interpret.

```{r LASSO, warning=FALSE}
#LASSO
# take models 5 (the best) and model 6 (overfit)
vars_model_5 <- c("price", basic_lev,basic_add,reviews,poly_lev,X1,amenities)
vars_model_6 <- c("price", basic_lev,basic_add,reviews,poly_lev,X1,amenities,X2)

# Set lasso tuning parameters
train_control <- trainControl(method = "cv", number = n_folds)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))

# Use model 6 
formula <- formula(paste0("price ~ ", paste(setdiff(vars_model_6, "price"), collapse = " + ")))

set.seed(20180124)
lasso_model <- caret::train(formula,
                      data = data_work,
                      method = "glmnet",
                      preProcess = c("center", "scale"),
                      trControl = train_control,
                      tuneGrid = tune_grid,
                    na.action=na.exclude)

print(lasso_model$bestTune$lambda)

lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `s1`)  # the column has a name "s1", to be renamed

print(lasso_coeffs)

lasso_coeffs_nz<-lasso_coeffs %>%
  filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))

# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) %>%
  dplyr::select(RMSE)
print(lasso_cv_rmse[1, 1])


```


### Diagnostics
After choosing M5 as the best model, I want to evaluate the actual prediction with post-prediction diagnostics. First, I calculate the RMSE of this model in the Holdout set. The RMSE is lower than in the cross validation. I also look at the resulting RMSE in the Holdout set when using LASSO. It is again lower than the cross validation, quite close to that from M5. Therefore, I perform the diagnostic analysis on M5.
From the first graph, it can be noticed that most of the low prices are quite well predicted, but the high prices are wrongly predicted. In the bar chart of price variation by size, it emerges that the model generates very wide 80% PI intervals. Therefore, predictions are quite imprecise.

```{r diagnostics, warning=FALSE}
#Diagnostics on M5
#model2_level <- model_results_cv[["modellev2"]][["model_work_data"]]
model5_level <- model_results_cv[["modellev5"]][["model_work_data"]]
# look at holdout RMSE
model5_level_work_rmse <- mse_lev(predict(model5_level, newdata = data_work), data_work[,"price"])**(1/2)
model5_level_holdout_rmse <- mse_lev(predict(model5_level, newdata = data_holdout), data_holdout[,"price"])**(1/2)
model5_level_holdout_rmse

#look at holdout RMSE using LASSO
model_lasso_holdout_rmse <- mse_lev(predict(lasso_model, newdata = data_holdout), data_holdout[,"price"])**(1/2)
model_lasso_holdout_rmse
#LASSO gives a slightly lower RMSE

# Figures for fitted vs actual outcome variables
# Target variable
Ylev <- data_holdout[["price"]]

meanY <-mean(Ylev)
sdY <- sd(Ylev)
meanY_m2SE <- meanY -1.96 * sdY
meanY_p2SE <- meanY + 1.96 * sdY
Y5p <- quantile(Ylev, 0.05, na.rm=TRUE)
Y95p <- quantile(Ylev, 0.95, na.rm=TRUE)

# Predicted values
predictionlev_holdout_pred <- as.data.frame(predict(model5_level, newdata = data_holdout, interval="predict")) %>% rename(pred_lwr = lwr, pred_upr = upr)
predictionlev_holdout_conf <- as.data.frame(predict(model5_level, newdata = data_holdout, interval="confidence")) %>% rename(conf_lwr = lwr, conf_upr = upr)

predictionlev_holdout <- cbind(data_holdout[,c("price","n_accommodates")],
                               predictionlev_holdout_pred,
                               predictionlev_holdout_conf[,c("conf_lwr","conf_upr")])


# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_holdout[,"fit"] )
# Check the differences
d$elev <- d$ylev - d$predlev

# Plot predicted vs price
level_vs_pred <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev), color = color[1], size = 1,
             shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
  #geom_smooth(aes(y=ylev, x=predlev), method="lm", color=color[2], se=F, size=0.8, na.rm=T)+
  geom_segment(aes(x = 0, y = 0, xend = 400, yend =400), size=0.5, color=color[2], linetype=2) +
  coord_cartesian(xlim = c(0, 400), ylim = c(0, 400)) +
  scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 400), breaks=seq(0, 400, by=50)) +
  scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 400), breaks=seq(0, 400, by=50)) +
  labs(y = "Price (US dollars)", x = "Predicted price  (US dollars)") +
  theme_bg() 
level_vs_pred
save_fig("Airbnb-level-vs-pred", output, "small")


# Redo predicted values at 80% PI
predictionlev_holdout_pred <- as.data.frame(predict(model5_level, newdata = data_holdout, interval="predict", level=0.8)) %>%
  rename(pred_lwr = lwr, pred_upr = upr)
predictionlev_holdout_conf <- as.data.frame(predict(model5_level, newdata = data_holdout, interval="confidence", level=0.8)) %>%
  rename(conf_lwr = lwr, conf_upr = upr)

predictionlev_holdout <- cbind(data_holdout[,c("price","n_accommodates")],
                               predictionlev_holdout_pred,
                               predictionlev_holdout_conf[,c("conf_lwr","conf_upr")])

summary(predictionlev_holdout_pred)

predictionlev_holdout_summary <-
  predictionlev_holdout %>%
  group_by(n_accommodates) %>%
  dplyr::summarise(fit = mean(fit, na.rm=TRUE), pred_lwr = mean(pred_lwr, na.rm=TRUE), pred_upr = mean(pred_upr, na.rm=TRUE),
            conf_lwr = mean(conf_lwr, na.rm=TRUE), conf_upr = mean(conf_upr, na.rm=TRUE))

kable(x = predictionlev_holdout_summary, format = "latex", booktabs=TRUE,  digits = 3, row.names = FALSE,
      linesep = "", col.names = c("Accomodates","Prediction","Pred. interval lower",
                                  "Pred. interval upper","Conf.interval lower","Conf.interval upper")) %>%
  cat(.,file= paste0(output, "modellev7_holdout_summary.tex"))


CI_n_accomodate <- ggplot(predictionlev_holdout_summary, aes(x=factor(n_accommodates))) +
  geom_bar(aes(y = fit ), stat="identity",  fill = color[1], alpha=0.7 ) +
  geom_errorbar(aes(ymin=pred_lwr, ymax=pred_upr, color = "Pred. interval"),width=.2) +
  #geom_errorbar(aes(ymin=conf_lwr, ymax=conf_upr, color = "Conf. interval"),width=.2) +
  scale_y_continuous(name = "Predicted price (US dollars)") +
  scale_x_discrete(name = "Accomodates (Persons)") +
  scale_color_manual(values=c(color[2], color[2])) +
  theme_bg() +
  theme(legend.title= element_blank(),legend.position="none")
CI_n_accomodate
save_fig("airbnb-ci-n-accomodate", output, "small")

```

## Random forest
This is an ensemble method that combines the results of hundreds of regression trees. One of the great advantages of Random forest is that variable selection, interaction picking, and functional form decisions are not needed.
### Set up
I keep the same working data and holdout set, same combinations of variables and I predict 3 models with RF:
- RF1. basic vars
- RF2. RF1 + reviews + amenities (benchmark model, similar to M5 but no interactions or polynomials)
- RFE. equal to M6 (for LASSO)
```{r RF setup}

#use same train(work) and holdout

dim(data_work)
dim(data_holdout)

#basic_lev
#basic_add
basic_vars <- c(basic_lev,basic_add)
#reviews
#amenities
#X1,X2

predictors_1 <- c(basic_vars) #RF1
predictors_2 <- c(basic_vars, reviews, amenities) #RF2
predictors_E <- c(basic_vars, reviews, X1, amenities, X2) #RFE

```

## Cross-validation
I perform cross-validated RF using RF1 and RF2, with imputed parameters and autotuning. The parameters to be chosen, or to be autotuned, are 
- number of bootstrap samples (500)
- number of variables considered in each split (decorrelation - square root of total number of varibles - try 8 10 12)
- minimum number of observations in terminal nodes of each tree (stopping rule - 5 10 15)
It emerges that the choice of the parameters do not matter much, but the best combination for the benchmark model (RF2) is 5 observations in terminal nodes and 10 variables at each split. The resulting RSME is 99.7. The autotuned RF performs worse.

```{r RF CV}

# do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)


# set tuning
tune_grid <- expand.grid(
  .mtry = c(5, 7, 9),
  .splitrule = "variance",
  .min.node.size = c(5, 10)
)


# simpler model for model A (1)
set.seed(20180124)
system.time({
rf_model_1 <- train(
  formula(paste0("price ~", paste0(predictors_1, collapse = " + "))),
  data = data_work,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)
})
rf_model_1

# set tuning for benchamrk model (2)
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(5, 10, 15)
)

set.seed(20180124)
system.time({
rf_model_2 <- train(
  formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
  data = data_work,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)
})

rf_model_2

# auto tuning first
 set.seed(20180124)
 system.time({
   rf_model_2auto <- train(
     formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
     data = data_work,
     method = "ranger",
     trControl = train_control,
     importance = "impurity"
   )
 })
 rf_model_2auto 
#rf_model_2auto <-rf_model_2

 
# evaluate random forests 

results <- resamples(
  list(
    model_1  = rf_model_1,
    model_2  = rf_model_2,
    model_2b = rf_model_2auto
  )
)
summary(results)


# Save outputs 

# Show Model 2B rmse shown with all the combinations
rf_tuning_modelB <- rf_model_2$results %>%
  dplyr::select(mtry, min.node.size, RMSE) %>%
  dplyr::rename(nodes = min.node.size) %>%
  spread(key = mtry, value = RMSE)

kable(x = rf_tuning_modelB, format = "latex", digits = 2, caption = "CV RMSE") %>%
  add_header_above(c(" ", "vars" = 3)) %>%
  cat(.,file= paste0(output,"rf_tuning_modelB.tex"))


# Turning parameter choice 1
result_1 <- matrix(c(
                     rf_model_1$finalModel$mtry,
                     rf_model_2$finalModel$mtry,
                     rf_model_2auto$finalModel$mtry,
                     rf_model_1$finalModel$min.node.size,
                     rf_model_2$finalModel$min.node.size,
                     rf_model_2auto$finalModel$min.node.size

                     ),
                    nrow=3, ncol=2,
                    dimnames = list(c("Model A", "Model B","Model B auto"),
                                    c("Min vars","Min nodes"))
                   )
kable(x = result_1, format = "latex", digits = 3) %>%
  cat(.,file= paste0(output,"rf_models_turning_choices.tex"))

# Turning parameter choice 2
result_2 <- matrix(c(mean(results$values$`model_1~RMSE`),
                     mean(results$values$`model_2~RMSE`),
                     mean(results$values$`model_2b~RMSE`)
),
nrow=3, ncol=1,
dimnames = list(c("Model A", "Model B","Model B auto"),
                c(results$metrics[2]))
)


kable(x = result_2, format = "latex", digits = 3) %>%
   cat(.,file= paste0(output,"rf_models_rmse.tex"))


```


## Diagnostics 
Random forest cannot be interepreted as it is. Diagnostics are possible on the holdout set:
1. Variable importance plot - shows average improvements in fit when variables are used to split the data
 - top 10
 - grouping factor variables
2. Partial dependence plot - shows how average predicted price differs for different values of a variable conditional on all other predictors
3. Subsample performance - for external validity
The most important variables are number of bathrooms, days in operations, municipi, number of accommodates. Moverover, conditionally on the rest, the average predicted price seems to be close to constant across Municipi, while there is a particular relation between average predicted prices and number or bathrooms, which looks like half of a parabola, i.e. it is far from being linear. Finally, the prediction works best (lower RMSE/mean(price)) for apartment size (number of accommodates), municipi, and entire places, not for private or shared rooms. Hence, the our prediction would be best used for the former types of observations only, while more information is needed to be able to make relevant decisions based on the less well predicted groups.

```{r}

# Variable Importance Plots 
# first need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}


# variable importance plot
# 1) full varimp plot, full
# 2) varimp plot , top 10
# 3) varimp plot grouped
# 4) varimp plot  w copy, top 10


rf_model_2_var_imp <- importance(rf_model_2$finalModel)/1000
rf_model_2_var_imp_df <-
  data.frame(varname = names(rf_model_2_var_imp),imp = rf_model_2_var_imp) %>%
  mutate(varname = gsub("f_municipio", "Borough:", varname) ) %>%
  mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))



# 1) full varimp plot, above a cutoff

# to have a quick look
plot(varImp(rf_model_2))

cutoff = 600
rf_model_2_var_imp_plot <- ggplot(rf_model_2_var_imp_df[rf_model_2_var_imp_df$imp>cutoff,],
                                  aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color=color[1], size=1.5) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=1) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bg() +
  theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
        axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))
rf_model_2_var_imp_plot
#save_fig("rf_varimp1",output, "large")
save_fig("Airbnb-rf-varimp-base",output, "large")


# 2) full varimp plot, top 10 only

# have a version with top 10 vars only
rf_model_2_var_imp_plot_b <- ggplot(rf_model_2_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color=color[1], size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bg() +
  theme(axis.text.x = element_text(size=4), axis.text.y = element_text(size=4),
        axis.title.x = element_text(size=4), axis.title.y = element_text(size=4))
rf_model_2_var_imp_plot_b
#save_fig("rf_varimp1_b",output, "small")
save_fig("Airbnb-rf-varimp-top10",output, "small")



# 3) varimp plot grouped

# grouped variable importance - keep binaries created off factors together

varnames <- rf_model_2$finalModel$xNames
f_municipio_varnames <- grep("f_municipio",varnames, value = TRUE)
f_room_type_varnames <- grep("f_room_type",varnames, value = TRUE)

groups <- list(f_municipio=f_municipio_varnames,
               f_room_type = f_room_type_varnames,
               f_bathroom = "n_bathroom",
               n_days_since = "n_days_since",
               n_accommodates = "n_accommodates",
               n_beds = "n_beds",
               f_minimum_nights = "f_minimum_nights",
               flag_days_since = "flag_days_since")

rf_model_2_var_imp_grouped <- group.importance(rf_model_2$finalModel, groups)
rf_model_2_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_2_var_imp_grouped),
                                            imp = rf_model_2_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_2_var_imp_grouped_plot <-
  ggplot(rf_model_2_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color=color[1], size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
theme_bg() +
  theme(axis.text.x = element_text(size=4), axis.text.y = element_text(size=4),
        axis.title.x = element_text(size=4), axis.title.y = element_text(size=4))
rf_model_2_var_imp_grouped_plot
save_fig("Airbnb-rf-varimp-group",output, "small")



# Partial Dependence Plots 

pdp_f_muni <- pdp::partial(rf_model_2, pred.var = "f_municipio", pred.grid = distinct_(data_holdout, "f_municipio"), train = data_work)
pdp_f_muni_plot <- pdp_f_muni %>%
   autoplot( ) +
  geom_point(color=color[1], size=2) +
  ylab("Predicted price") +
  xlab("Municipi") +
  scale_y_continuous(limits=c(12,150), breaks=seq(120,150, by=5)) +
  theme_bg()
pdp_f_muni_plot
save_fig("Airbnb-rf-pdp-f-municipi", output, "small")


pdp_n_bathrooms <- pdp::partial(rf_model_2, pred.var = "n_bathrooms", pred.grid = distinct_(data_holdout, "n_bathrooms"), train = data_work)
pdp_n_bathrooms_plot <- pdp_n_bathrooms %>%
   autoplot( ) +
  geom_point(color=color[1], size=2) +
  geom_line(color=color[1], size=1) +
  ylab("Predicted price") +
  xlab("Number of bathrooms") +
  scale_x_continuous(limit=c(1,4), breaks=seq(1,4,1))+
theme_bg()
pdp_n_bathrooms_plot
save_fig("Airbnb-rf-pdp-bathrooms", output, "small")

# Subsample performance: RMSE / mean(y) 
# On the holdout set

# ---- cheaper or more expensive flats - not used in book
data_holdout_w_prediction <- data_holdout %>%
  mutate(predicted_price = predict(rf_model_2, newdata = data_holdout))

######### create nice summary table of heterogeneity
a <- data_holdout_w_prediction %>%
  mutate(is_low_size = ifelse(n_accommodates <= 3, "small apt", "medium apt")) %>%
  group_by(is_low_size) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )

#3 richest, 3 poorest
b <- data_holdout_w_prediction %>%
  filter(f_municipio %in% c("Municipio1", "Municipio3", "Municipio2", "Municipio8", "Municipio7", "Municipio9")) %>%
  group_by(f_municipio) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )

c <- data_holdout_w_prediction %>%
  filter(f_room_type %in% c("Entire home/apt", "Private room", "Shared room")) %>%
  group_by(f_room_type) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )


d <- data_holdout_w_prediction %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )

# Save output
colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")

line1 <- c("Type", "", "", "")
line2 <- c("Apartment size", "", "", "")
line3 <- c("Borough", "", "", "")

result_3 <- rbind(line2, a, line1, c, line3, b, d) %>%
  transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
            `RMSE/price` = as.numeric(`RMSE/price`))

options(knitr.kable.NA = '')
kable(x = result_3, format = "latex", booktabs=TRUE, linesep = "",digits = c(0,2,1,2), col.names = c("","RMSE","Mean price","RMSE/price")) %>%
  cat(.,file= paste0(output, "performance_across_subsamples.tex"))
options(knitr.kable.NA = NULL)


```


## Horserace
Comparing OLS, LASSO and RF RSME 
- OLS and RF on bechmark model
- LASSO on extended benchmark model (including interactions)
RF better than others 
```{r, warning=FALSE}

# OLS with dummies for area
# using model B

set.seed(20180124)
system.time({
ols_model <- train(
  formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
  data = data_train,
  method = "lm",
  trControl = train_control
)
})

ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))

# * LASSO
# using extended model w interactions

set.seed(20180124)
system.time({
lasso_model2 <- train(
  formula(paste0("price ~", paste0(predictors_E, collapse = " + "))),
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
  trControl = train_control
)
})

lasso_coeffs2 <- coef(
    lasso_model2$finalModel,
    lasso_model2$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(lasso_coefficient = `s1`)  # the column has a name "1", to be renamed

lasso_coeffs_non_null2 <- lasso_coeffs2[!lasso_coeffs2$lasso_coefficient == 0,]

regression_coeffs <- merge(ols_model_coeffs_df, lasso_coeffs_non_null2, by = "variable", all=TRUE)
regression_coeffs %>%
  write.csv(file = paste0(output, "regression_coeffs.csv"))

# ---- compare these models

final_models <-
  list("OLS" = ols_model,
  "LASSO (model w/ interactions)" = lasso_model2,
  "Random forest (smaller model)" = rf_model_1,
  "Random forest" = rf_model_2,
  "Random forest (auto tuned)" = rf_model_2auto)

results <- resamples(final_models) %>% summary()


# Save output 
# Model selection is carried out on this CV RMSE

result_4 <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")

kable(x = result_4, format = "latex", digits = 3, booktabs=TRUE, linesep = "") %>%
  cat(.,file= paste0(output,"horse_race_of_models_cv_rmse.tex"))


# evaluate preferred model on the holdout set

result_5 <- map(final_models, ~{
  RMSE(predict(.x, newdata = data_holdout), data_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")

kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "") %>%
  cat(.,file= paste0(output,"horse_race_of_models_houldout_rmse.tex"))
```


